I"á<p></p>
<p><button type="button" class="btn btn-secondary btn-sm" onclick=" relocate_home()" style="width:120px;height:40px;border:2px blue none;background-color:lightgrey;">Paper Link</button></p>

<script>
function relocate_home()
{
     location.href = "https://garylkl.github.io/pdf_files/bdad_final.pdf";
} 
</script>

<h2 id="introduction">Introduction</h2>

<p>In this project, we utilized collaborative filtering to build a recommender system on the Goodreads dataset. In particular, we used Alternating Least Square (ALS) and Spark to train our models, and attained RMSE of 1.6908, PrecisionAt of 0.770 on the test set. Furthermore, we implemented fast searching that increased the speed of the query of 10 users from 86 seconds to 0.33 seconds. We also explored the visualization of the learned space in this paper.</p>

<h2 id="architecture">Architecture</h2>

<p>The description of the implementation is divided into two parts: (1) Data Preprocessing; (2) Modeling with Latent Factor Model. All work is done as Spark jobs on on Dumbo, the Hadoop cluster at New York University as the original dataset is large (4.1GB).  Besides, we do fast-queries and build Latent Factor Model in PySpark, which is the Python API for Spark. The whole data science process is generally separated into three stages from data pre-processing to model evaluation. The code for this project is available online.</p>

<h2 id="fast-search">Fast Search</h2>

<p>Query time is crucial in building a recommender system. Therefore, we compared query time of three methods to associate 500 books with each user.</p>

<ul>
  <li><strong>Naive Approach:</strong> Bring user latent factors and item latent factors into memory of driver machine and use heapq(priority queue) to find the top 500 books for each user.</li>
  <li><strong>Spark Approach:</strong> Since computing dot product between user latent factor and item latent factor is parallelizable, the second method take advantages of spark to do the computation.</li>
  <li><strong>Annoy Approach:</strong> For each user, the best books to recommend are the books that are the nearest neighbors of this user. Therefore, we can build a spatial data structure in the space of all book latent factors after which we can easily find the most nearest neighbors of user at query time. The <a href="https://github.com/spotify/annoy">Annoy library</a> did a good job at implementing it and it reduced the query time a lot.</li>
</ul>

<h2 id="keyword-recommendation">Keyword Recommendation</h2>

<p>Given the description of a Kickstarter campaign as a query document, we would like to the retrieve the most similar documents from the Amazon dataset. The most straightforward idea is to compare its cosine similarity to each document one by one. However, this is compute-intensive. Therefore, we adopt locality sensitive hashing (LSH) to map each 200d GloVe embedding to a lower dimensional binary vector. The close documents in the original vector space will very likely be close in the new space. And we can reduce a great amount of search time.</p>

<p>Finally, we select the high-rated documents within top-k similar documents and compute the mean of their TF-IDF vector. If a word is more important, it will have a higher value in the resulting mean vector.</p>
:ET