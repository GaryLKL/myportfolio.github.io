I"’<p></p>
<p><button type="button" class="btn btn-secondary btn-sm" onclick=" relocate_home()" style="width:120px;height:40px;border:2px blue none;background-color:lightgrey;">Paper Link</button></p>

<script>
function relocate_home()
{
     location.href = "https://garylkl.github.io/pdf_files/ml_final.pdf";
} 
</script>

<h2 id="introduction">Introduction</h2>

<p>Online reviews provide customers and business owners a good evaluation metric for the target products or services. Therefore, knowing the authenticity of reviews is essential, because they may affect how we make a decision. In this work, we tackled the fake review detection problem by designing new features, re-sampling training data and ensembling different machine learning models. Our best performance XGBoost model achieved 0.404 Average Precision and 0.862 AUC score on test data.</p>

<figure class="third ">
  
    
      <a href="/images/ml/review-length.jpg">
          <img src="/images/ml/review-length.jpg" alt="placeholder image 1" />
      </a>
    
  
    
      <a href="/images/ml/rating-label.jpg">
          <img src="/images/ml/rating-label.jpg" alt="placeholder image 2" />
      </a>
    
  
    
      <a href="/images/ml/table.jpg">
          <img src="/images/ml/table.jpg" alt="placeholder image 3" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>

<h2 id="feature-engineering">Feature Engineering</h2>

<h3 id="1-linguistic-features">1. Linguistic Features</h3>

<ul>
  <li><strong>HTML Tags and URL:</strong> Web-scraped data may contain HTML tags which are not useful in text data, so we use a Python library, BeautifulSoup, to remove them.</li>
  <li><strong>Accented characters:</strong> Since we are dealing with English reviews, accented characters, e.g. cafÃ©, can be confusing. In this project, we convert all accented characters into normal English characters, e.g. cafe.</li>
  <li><strong>Punctuation \&amp; Special Characters:</strong> The Punctuation marks, such as question marks and quotes, inclusive of any special characters are also removed from the text data</li>
  <li><strong>Stopwords:</strong> In the Spark NLP pipeline, we use the pre-defined stopwords collection from NLTK to extract the words which do not add much value in the article.</li>
  <li><strong>Lemmatization \&amp; Lower Case:</strong> One word can be written in different form with different tenses. Lemmatization helps us unifying them into a single format. After that, we convert all tokens into the lower case.</li>
</ul>

<h3 id="2-review-centric-features">2. Review-centric Features</h3>
<ul>
  <li>Review length</li>
  <li>Absolute rating deviation from productâ€™s average rating</li>
  <li><strong>Earlier time review:</strong> Whether the current review is written within the T=70 months from the first review for this product. Itâ€™s reasonable for a spammer to post a review as early as possible to increase their chances of being viewed.</li>
  <li><strong>Whether a review is written on holiday or not [new]:</strong> Itâ€™s more effective to write fake reviews on holidays, since people are more likely to go out on these days.</li>
  <li>Whether a review is written on weekend or not</li>
</ul>

<h3 id="3-reviewer-centric-features">3. Reviewer-centric Features</h3>

<ul>
  <li>Number of reviews per user</li>
  <li>Max/avg reviews per user per day</li>
  <li>Percentage of positive/negative reviews per user</li>
  <li><strong>Burstiness:</strong> Whether all reviews from a user are written within T=28 days.</li>
  <li><strong>Avg rating deviation:</strong> How different are oneâ€™s reviews from others.</li>
  <li>Avg/Var review length per user</li>
  <li><strong>Avg content similarity (n-gram):</strong> Compute the average of pairwise jaccard similarities among userâ€™s reviews. A review is represented as a 500d uni-gram vector.</li>
  <li><strong>Avg content similarity (GloVe):</strong> Compute the average of pairwise cosine similarities among userâ€™s reviews. A review is represented as a 50d GloVe vector. We argue that GloVe can better embed the meaning of the text into a smooth manifold. As a result, it can capture the information where uni-gram cannot.</li>
</ul>

<h2 id="imbalanced-data">Imbalanced Data</h2>

<ul>
  <li><strong>Random under-sampling (RUS):</strong> Only a subset of samples from the majority class are used. Some important information in the out-of-sample data may be lost.</li>
  <li><strong>Random over-sampling (ROS):</strong> Each sample in the minority class might be sampled for multiple times. However, replicating too much information may result in over-fitting.</li>
  <li><strong>SMOTE (Synthetic Minority Oversampling Technique):</strong> Basically, the technique generates new samples by interpolating of its nearest neighbors.</li>
  <li><strong>ROS + RUS:</strong> This method can benefit from the advantages of both approaches.</li>
  <li><strong>SMOTE + RUS:</strong> For the model with only the linguistic features, we use this method to create a dataset with 50,000 in both of the target classes.</li>
  <li><strong>Ensemble of RUS:</strong> Perform RUS several times, each time with a different subset of samples from the majority class. Make the final prediction by voting.</li>
</ul>
:ET